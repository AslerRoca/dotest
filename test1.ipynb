import torch
import torch.nn as nn
from torch.optim import AdamW
from torch.optim.lr_scheduler import StepLR

class EncoderOnlyTransformer(nn.Module):
    def __init__(self, input_dim=8192, output_dim=18, d_model=256, n_heads=8, num_layers=8, dim_feedforward=1024, dropout=0.1):
        super().__init__()

        # Project the single input vector to d_model
        self.input_proj = nn.Linear(input_dim, d_model)

        # Create a single learnable query token: [1, d_model]
        self.query_token = nn.Parameter(torch.randn(1, d_model))

        # Sequence length = 2: one input token + one query token
        seq_len = 2
        self.positional_embedding = nn.Parameter(torch.randn(1, seq_len, d_model))

        # Transformer Encoder
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,
                                                   nhead=n_heads,
                                                   dim_feedforward=dim_feedforward,
                                                   dropout=dropout,
                                                   batch_first=True)
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

        # We want final shape [batch, 18, 3]
        # That means out_features = 18 * 3 = 54
        self.output_proj = nn.Linear(d_model, output_dim * 4)

    def forward(self, x):
        """
        x: [batch_size, 8192]
        Returns: [batch_size, 18, 3]
        """
        batch_size = x.size(0)

        # Project input: [batch, 8192] -> [batch, 1, d_model]
        inp = self.input_proj(x).unsqueeze(1)

        # Expand query token: [1, d_model] -> [batch, 1, d_model]
        query_token = self.query_token.unsqueeze(0).expand(batch_size, -1, -1)

        # Concatenate input token and query token: [batch, 2, d_model]
        combined = torch.cat([inp, query_token], dim=1)

        # Add positional embeddings
        pos_emb = self.positional_embedding[:, :combined.size(1), :]
        combined = combined + pos_emb

        # Encoder: [batch, 2, d_model]
        encoded = self.encoder(combined)

        # Use the query tokenâ€™s final representation: [batch, d_model]
        query_output = encoded[:, 1, :]

        # Project to 54 = 18*3 features: [batch, 54]
        out = self.output_proj(query_output)

        # Reshape to [batch, 18, 3]
        out = out.view(batch_size, 18, 4)

        return out
    
def to_one_hot(x):

    num_classes = 5  # since your classes are {0,1,2,3}

    x=x+1
    # Create a zero tensor for one-hot encoding: [batch, length, classes]
    one_hot = torch.zeros(x.size(0), x.size(1), num_classes,device=x.device)

    # Use scatter_ to fill in the one-hot encoding
    # scatter_ takes indices along a dimension and sets values (1 here) at those indices
    one_hot.scatter_(2, x.unsqueeze(2).long(), 1)

    return one_hot[:, :, 1:]
    # return one_hot[:, :, :]

class CorrelationLoss(nn.Module):
    def __init__(self) -> None:
        super(CorrelationLoss,self).__init__()
    
    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        x = y_pred
        y = y_true
        mx = torch.mean(x)
        my = torch.mean(y)
        xm, ym = x - mx, y - my
        r_num = torch.sum(xm * ym)
        r_den = torch.sqrt(torch.sum(xm*xm)*torch.sum(ym*ym))
        r = r_num / r_den
        r = torch.clamp(r, -1.0, 1.0)
        return 1 - r

class CombinedLoss(nn.Module):
    def __init__(self, lambda_weight: float = 0.9) -> None:
        super(CombinedLoss, self).__init__()
        self.lambda_weight = lambda_weight
        self.cross_entropy_loss = nn.CrossEntropyLoss(ignore_index=0)
        self.correlation_loss = CorrelationLoss()

    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:
        # Expect: y_pred [batch, length, classes], y_true [batch, length]
        ce_loss = self.cross_entropy_loss(y_pred.permute(0,2,1), y_true)
        corr_loss = self.correlation_loss(y_pred.argmax(2).float(), y_true.float())
        combined = (1 - self.lambda_weight) * ce_loss + self.lambda_weight * corr_loss
        return combined, ce_loss, corr_loss

class NoamOpt:
    """
    Noam scheduler as described in the "Attention is All You Need" paper.
    """
    def __init__(self, d_model, warmup_steps, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup_steps
        self.d_model = d_model
        self._rate = 0

    def step(self):
        # Update step count
        self._step += 1
        # Compute the learning rate
        rate = (self.d_model ** (-0.5) *
                min(self._step ** (-0.5), self._step * (self.warmup ** (-1.5))))
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def zero_grad(self):
        self.optimizer.zero_grad()




device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

d_model = 2048
warmup_steps = 10000
num_epochs = 250
batch_size= 10
model = EncoderOnlyTransformer().to(device)
criterion = CombinedLoss(lambda_weight=0.9)
# Define your loaders:
# Since we treat output as a single-step classification with 18 classes, enc_outputs is [batch]
# and y_pred is [batch,18], we interpret this as [batch,1,18] for CE.
# Ensure enc_outputs are integers for CE and possibly floats for correlation (we will cast).
loader_train = Data.DataLoader(dataset=MyDataSet(np.array(features_train),np.array(gt_train)),batch_size=batch_size,shuffle=True) # your DataLoader
loader_val = Data.DataLoader(dataset=MyDataSet(np.array(features_validation),np.array(gt_validation)),batch_size=batch_size,shuffle=True)# your validation DataLoader
# # Optimizer and Noam schedule
base_optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9,0.98), eps=1e-9)
scheduler = NoamOpt(d_model, warmup_steps, base_optimizer)
# # Use AdamW as the optimizer
# base_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.0001)

# # Learning rate scheduler (optional, e.g., linear decay, cosine annealing, etc.)
# scheduler = torch.optim.lr_scheduler.StepLR(base_optimizer, step_size=200, gamma=0.5)  # Adjust parameters as needed

train_losses = []
val_losses = []
learning_rate=[]


################ Use Noam
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

d_model = 2048
warmup_steps = 10000
num_epochs = 250
batch_size= 10
model = EncoderOnlyTransformer().to(device)
criterion = CombinedLoss(lambda_weight=0.9)
# Define your loaders:
# Since we treat output as a single-step classification with 18 classes, enc_outputs is [batch]
# and y_pred is [batch,18], we interpret this as [batch,1,18] for CE.
# Ensure enc_outputs are integers for CE and possibly floats for correlation (we will cast).
loader_train = Data.DataLoader(dataset=MyDataSet(np.array(features_train),np.array(gt_train)),batch_size=batch_size,shuffle=True) # your DataLoader
loader_val = Data.DataLoader(dataset=MyDataSet(np.array(features_validation),np.array(gt_validation)),batch_size=batch_size,shuffle=True)# your validation DataLoader
# # Optimizer and Noam schedule
base_optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9,0.98), eps=1e-9)
scheduler = NoamOpt(d_model, warmup_steps, base_optimizer)
# # Use AdamW as the optimizer
# base_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.0001)

# # Learning rate scheduler (optional, e.g., linear decay, cosine annealing, etc.)
# scheduler = torch.optim.lr_scheduler.StepLR(base_optimizer, step_size=200, gamma=0.5)  # Adjust parameters as needed

train_losses = []
val_losses = []
learning_rate=[]

for epoch in range(num_epochs):
    model.train()
    train_loss_sum = 0.0
    train_batches = 0
    # Training loop with tqdm
    with tqdm(loader_train, desc=f"Epoch {epoch+1} [Train]", unit="batch") as t:
        for enc_inputs, enc_outputs in t:
            enc_inputs = enc_inputs.float().to(device)    # [batch,8192]
            enc_outputs=enc_outputs.to(device).long()+1
            enc_outputs_onehot=to_one_hot(enc_outputs)
            enc_outputs_onehot = enc_outputs_onehot.to(device)          # [batch,18,3]
            base_optimizer.zero_grad()
            y_pred = model(enc_inputs)                   # [batch,18, 3]
            # Reshape for CE: [batch,18] -> [batch,1,18]
            combined_loss, ce_loss, corr_loss = criterion(y_pred, enc_outputs)
            combined_loss.backward()
            scheduler.step()
            train_loss_sum += combined_loss.item()
            train_batches += 1
            t.set_postfix(loss=combined_loss.item())
    avg_train_loss = train_loss_sum / train_batches
    train_losses.append(avg_train_loss)
    current_lr=scheduler._rate
    # for param_group in base_optimizer.param_groups:
        
    #     current_lr = param_group['lr']

    for name,param in model.named_parameters():
        if param.requires_grad:
            print(f"Parameter: {name}, Gradient Norm: {param.grad.norm().item()}")    
    learning_rate.append(current_lr)
    # Validation loop
    model.eval()
    val_loss_sum = 0.0
    val_batches = 0
    with torch.no_grad():
        with tqdm(loader_val, desc=f"Epoch {epoch+1} [Val]", unit="batch") as t:
            for enc_inputs, enc_outputs in t:
                enc_inputs = enc_inputs.float().to(device)
                enc_outputs=enc_outputs.to(device).long()+1
                enc_outputs_onehot=to_one_hot(enc_outputs)
                enc_outputs_onehot = enc_outputs_onehot.to(device)
                y_pred = model(enc_inputs) 
                combined_loss, ce_loss, corr_loss = criterion(y_pred, enc_outputs)
                val_loss_sum += combined_loss.item()
                val_batches += 1
                t.set_postfix(loss=combined_loss.item())
    avg_val_loss = val_loss_sum / val_batches
    val_losses.append(avg_val_loss)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss:{avg_val_loss:.4f}, learning rate: {current_lr}")
    


############### use adam
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

d_model = 2048
warmup_steps = 10000
num_epochs = 250
batch_size= 10
model = EncoderOnlyTransformer().to(device)
criterion = CombinedLoss(lambda_weight=0.9)
# Define your loaders:
# Since we treat output as a single-step classification with 18 classes, enc_outputs is [batch]
# and y_pred is [batch,18], we interpret this as [batch,1,18] for CE.
# Ensure enc_outputs are integers for CE and possibly floats for correlation (we will cast).
loader_train = Data.DataLoader(dataset=MyDataSet(np.array(features_train),np.array(gt_train)),batch_size=batch_size,shuffle=True) # your DataLoader
loader_val = Data.DataLoader(dataset=MyDataSet(np.array(features_validation),np.array(gt_validation)),batch_size=batch_size,shuffle=True)# your validation DataLoader
# # Optimizer and Noam schedule
# base_optimizer = torch.optim.AdamW(model.parameters(), lr=0, betas=(0.9,0.98), eps=1e-9)
# scheduler = NoamOpt(d_model, warmup_steps, base_optimizer)
# Use AdamW as the optimizer
base_optimizer = torch.optim.AdamW(model.parameters(), lr=1e-8, betas=(0.9, 0.98), eps=1e-8, weight_decay=0.0001)

# Learning rate scheduler (optional, e.g., linear decay, cosine annealing, etc.)
scheduler = torch.optim.lr_scheduler.StepLR(base_optimizer, step_size=1, gamma=1.005)  # Adjust parameters as needed

train_losses = []
val_losses = []
learning_rate=[]

for epoch in range(num_epochs):
    model.train()
    train_loss_sum = 0.0
    train_batches = 0
    # Training loop with tqdm
    with tqdm(loader_train, desc=f"Epoch {epoch+1} [Train]", unit="batch") as t:
        for enc_inputs, enc_outputs in t:
            enc_inputs = enc_inputs.float().to(device)    # [batch,8192]
            enc_outputs=enc_outputs.to(device).long()+1
            enc_outputs_onehot=to_one_hot(enc_outputs)
            enc_outputs_onehot = enc_outputs_onehot.to(device)          # [batch,18,3]
            base_optimizer.zero_grad()
            y_pred = model(enc_inputs)                   # [batch,18, 3]
            # Reshape for CE: [batch,18] -> [batch,1,18]
            combined_loss, ce_loss, corr_loss = criterion(y_pred, enc_outputs)
            combined_loss.backward()
            scheduler.step()
            train_loss_sum += combined_loss.item()
            train_batches += 1
            t.set_postfix(loss=combined_loss.item())
    avg_train_loss = train_loss_sum / train_batches
    train_losses.append(avg_train_loss)
    # current_lr=scheduler._rate
    for param_group in base_optimizer.param_groups:
        
        current_lr = param_group['lr']

    # for name,param in model.named_parameters():
    #     if param.requires_grad:
    #         print(f"Parameter: {name}, Gradient Norm: {param.grad.norm().item()}")    
    learning_rate.append(current_lr)
    # Validation loop
    model.eval()
    val_loss_sum = 0.0
    val_batches = 0
    with torch.no_grad():
        with tqdm(loader_val, desc=f"Epoch {epoch+1} [Val]", unit="batch") as t:
            for enc_inputs, enc_outputs in t:
                enc_inputs = enc_inputs.float().to(device)
                enc_outputs=enc_outputs.to(device).long()+1
                enc_outputs_onehot=to_one_hot(enc_outputs)
                enc_outputs_onehot = enc_outputs_onehot.to(device)
                y_pred = model(enc_inputs) 
                combined_loss, ce_loss, corr_loss = criterion(y_pred, enc_outputs)
                val_loss_sum += combined_loss.item()
                val_batches += 1
                t.set_postfix(loss=combined_loss.item())
    avg_val_loss = val_loss_sum / val_batches
    val_losses.append(avg_val_loss)
    print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss:{avg_val_loss:.4f}, learning rate: {current_lr}")
    


